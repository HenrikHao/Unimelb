{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting track ids, album ids, artist ids and genre ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from trackdata.txt file\n",
    "with open('../data/landed/trackData1.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "# Split the data into lines\n",
    "lines = data.split('\\n')\n",
    "\n",
    "# Parse the data\n",
    "track_ids = []\n",
    "for line in lines:\n",
    "    # Split the line by '|'\n",
    "    elements = line.split('|')\n",
    "    if elements[0]:\n",
    "        track_ids.append(int(elements[0]))\n",
    "\n",
    "# Read the data from trackdata.txt file\n",
    "with open('../data/landed/albumData1.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "# Split the data into lines\n",
    "lines = data.split('\\n')\n",
    "\n",
    "# Parse the data\n",
    "album_ids = []\n",
    "for line in lines:\n",
    "    # Split the line by '|'\n",
    "    elements = line.split('|')\n",
    "    if elements[0]:\n",
    "        album_ids.append(int(elements[0]))\n",
    "\n",
    "# Read the data from trackdata.txt file\n",
    "with open('../data/landed/artistData1.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "# Split the data into lines\n",
    "lines = data.split('\\n')\n",
    "\n",
    "# Parse the data\n",
    "artist_ids = []\n",
    "for line in lines:\n",
    "    if line:\n",
    "        artist_ids.append(int(line))\n",
    "\n",
    "# Read the data from trackdata.txt file\n",
    "with open('../data/landed/genreData1.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "# Split the data into lines\n",
    "lines = data.split('\\n')\n",
    "\n",
    "# Parse the data\n",
    "genre_ids = []\n",
    "for line in lines:\n",
    "    if line:\n",
    "        genre_ids.append(int(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing User data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "input_file_path = '../data/landed/trainIdx1.txt'\n",
    "output_file_path = '../data/curated/song_ratings_train.csv'\n",
    "\n",
    "# Preprocess the file to associate user IDs correctly\n",
    "def preprocess_file(input_path, output_path):\n",
    "    with open(input_path, 'r') as infile, open(output_path, 'w') as outfile:\n",
    "        outfile.write(\"user_id,item_id,ratings\\n\")\n",
    "        current_user_id = None\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if \"|\" in line:\n",
    "                # Update the current user ID\n",
    "                current_user_id = line.split(\"|\")[0]\n",
    "            else:\n",
    "                # Write the user_id, track_id, and rating to the output file\n",
    "                parts = line.split()\n",
    "                outfile.write(f\"{current_user_id},{parts[0]},{parts[1]}\\n\")\n",
    "\n",
    "# Preprocess the input file\n",
    "preprocess_file(input_file_path, output_file_path)\n",
    "\n",
    "# File paths\n",
    "input_file_path = '../data/landed/validationIdx1.txt'\n",
    "output_file_path = '../data/curated/song_ratings_validation.csv'\n",
    "\n",
    "# Preprocess the input file\n",
    "preprocess_file(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CombineCSV\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Paths to your CSV files\n",
    "data1_path = \"../data/curated/song_ratings_train.csv\"\n",
    "data2_path = \"../data/curated/song_ratings_validation.csv\"\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "df1_spark = spark.read.csv(data1_path, header=True, inferSchema=True)\n",
    "df2_spark = spark.read.csv(data2_path, header=True, inferSchema=True)\n",
    "\n",
    "# Add a source and a sequence column to each DataFrame to preserve their original order\n",
    "df1_spark = df1_spark.withColumn(\"source\", F.lit(\"train\")).withColumn(\"sequence\", F.monotonically_increasing_id())\n",
    "df2_spark = df2_spark.withColumn(\"source\", F.lit(\"validation\")).withColumn(\"sequence\", F.monotonically_increasing_id())\n",
    "\n",
    "# Combine the DataFrames\n",
    "merged_df = df1_spark.unionByName(df2_spark)\n",
    "\n",
    "# Sort by user_id and source to ensure training entries come before validation entries, then by sequence\n",
    "merged_df = merged_df.orderBy([\"user_id\", \"source\", \"sequence\"])\n",
    "\n",
    "# Drop the extra columns used for sorting\n",
    "merged_df = merged_df.drop(\"source\", \"sequence\")\n",
    "\n",
    "# Add the four new columns based on whether item_id is in any of the lists\n",
    "merged_df = merged_df.withColumn('istrack', F.col('item_id').isin(track_ids).cast('integer'))\n",
    "merged_df = merged_df.withColumn('isalbum', F.col('item_id').isin(album_ids).cast('integer'))\n",
    "merged_df = merged_df.withColumn('isartist', F.col('item_id').isin(artist_ids).cast('integer'))\n",
    "merged_df = merged_df.withColumn('isgenre', F.col('item_id').isin(genre_ids).cast('integer'))\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.write.csv('../data/curated/combined_song_ratings', header=True, mode='overwrite')\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save as a single CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Combine CSV\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Directory containing the CSV files\n",
    "data_directory = \"../data/curated/combined_song_ratings\"\n",
    "\n",
    "# List all files in the directory and filter for CSV files, then sort them\n",
    "file_paths = sorted([os.path.join(data_directory, f) for f in os.listdir(data_directory) if f.endswith('.csv')])\n",
    "\n",
    "# Read and combine each CSV file in the sorted order\n",
    "combined_df = None\n",
    "for file_path in file_paths:\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    combined_df = df if combined_df is None else combined_df.unionByName(df)\n",
    "\n",
    "combined_df = combined_df.coalesce(1)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.write.csv('../data/development/combined_song_ratings.csv', header=True, mode='overwrite')\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "import os\n",
    "\n",
    "# Specify the directory where the CSV file is located\n",
    "directory = '../data/development/combined_song_ratings.csv'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Filter out the CSV file\n",
    "csv_file = [f for f in files if f.endswith('.csv')]\n",
    "\n",
    "\n",
    "old_name = os.path.join(directory, csv_file[0])\n",
    "new_name = os.path.join(directory, 'data.csv')\n",
    "    \n",
    "# Rename the file\n",
    "os.rename(old_name, new_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_remove_header(df: DataFrame, original_cols: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Renames columns in df to the specified original columns specified \n",
    "    in header row, and removes the header row. \n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input Spark DataFrame with default column names.\n",
    "    original_cols (list): A list of original column names to rename the DataFrame columns.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with renamed columns and without the header row.\n",
    "    \"\"\"\n",
    "    # Rename columns to original names\n",
    "    for i in range(0, len(df.columns)):\n",
    "        df = df.withColumnRenamed(f\"_c{i}\", original_cols[i])\n",
    "\n",
    "    # Remove the first row, assuming it's the header\n",
    "    df = df.filter(df[\"user_id\"] != \"user_id\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cast_columns_to_int(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Casts the 'ratings' and binary indicator columns to IntegerType.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input Spark DataFrame with columns to be cast.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with 'ratings' and binary columns cast to integers.\n",
    "    \"\"\"\n",
    "    # Cast the rating and binary variables to integers\n",
    "    df = df.withColumn(\"ratings\", F.col(\"ratings\").cast(IntegerType())) \\\n",
    "           .withColumn(\"istrack\", F.col(\"istrack\").cast(IntegerType())) \\\n",
    "           .withColumn(\"isalbum\", F.col(\"isalbum\").cast(IntegerType())) \\\n",
    "           .withColumn(\"isartist\", F.col(\"isartist\").cast(IntegerType())) \\\n",
    "           .withColumn(\"isgenre\", F.col(\"isgenre\").cast(IntegerType()))\n",
    "\n",
    "    return df\n",
    "\n",
    "def discretize_ratings(df: DataFrame, t1: int, t2: int) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Discretizes the 'ratings' column based on the given thresholds t1 and t2.\n",
    "    \n",
    "    Ratings will be categorized as:\n",
    "    - 0 <= rating < t1  -> Category 0\n",
    "    - t1 <= rating < t2  -> Category 1\n",
    "    - t2 <= rating <= 100 -> Category 2\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input Spark DataFrame containing the 'ratings' column.\n",
    "    t1 (int): The first threshold for discretizing ratings.\n",
    "    t2 (int): The second threshold for discretizing ratings.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with an additional column 'ratings_discretized'.\n",
    "    \"\"\"\n",
    "    # Create a new column 'ratings_discretized' based on the conditions\n",
    "    df_discretized = df.withColumn(\n",
    "        \"ratings_discretized\",\n",
    "        F.when((F.col(\"ratings\") >= 0) & (F.col(\"ratings\") < t1), 0)\n",
    "        .when((F.col(\"ratings\") >= t1) & (F.col(\"ratings\") < t2), 1)\n",
    "        .when((F.col(\"ratings\") >= t2) & (F.col(\"ratings\") <= 100), 2)\n",
    "        .otherwise(None)  # Handle any outliers, although ratings are assumed to be within 0-100\n",
    "    )\n",
    "    \n",
    "    return df_discretized\n",
    "\n",
    "\n",
    "def add_timestep(df):\n",
    "    \"\"\"\n",
    "    Adds a timestep column to df based on the order of appearance for each user.\n",
    "    The timestep will be sequential for EACH USER based on their row order.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input Spark DataFrame containing user ratings.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with an additional 'timestep' column.\n",
    "    \"\"\"\n",
    "    # Define the window specification to partition by user_id and preserve the row order\n",
    "    window_spec = Window.partitionBy(\"user_id\").orderBy(F.monotonically_increasing_id())\n",
    "    \n",
    "    # Add a column 'timestep' with the row number as the order of the rating for each user\n",
    "    df_with_timestep = df.withColumn(\"timestep\", F.row_number().over(window_spec))\n",
    "    \n",
    "    return df_with_timestep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_save = spark.read.csv(\"../data/development/combined_song_ratings.csv/data.csv\") # to be saved \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of original columns\n",
    "original_cols = [\"user_id\", \"item_id\", \"ratings\", \"istrack\", \"isalbum\", \"isartist\", \"isgenre\"]\n",
    "\n",
    "## If you want to directly save\n",
    "\n",
    "# 1. apply the rename and remove header function\n",
    "# 2. apply the function to cast columns to integers\n",
    "# 3. discretize ratings\n",
    "# 4. add timestep\n",
    "\n",
    "# Ensure that data is ordered by user_id and timestep\n",
    "#df_with_timestep_ordered = df_with_timestep.orderBy(\"user_id\", \"timestep\")\n",
    "\n",
    "# Apply all transformations in one chain --> more efficient\n",
    "df_save = (rename_and_remove_header(df_save, original_cols)\n",
    "           .transform(cast_columns_to_int)\n",
    "           .transform(lambda df: discretize_ratings(df, 30, 70))\n",
    "           .transform(add_timestep)\n",
    "           )  # Cache for reuse\n",
    "\n",
    "# add_timestep might display the sequnece of last user first\n",
    "# we'd prefer display sequences of users in order (userid=0,1,2...)\n",
    "df_save = df_save.orderBy(\"user_id\", \"timestep\")\n",
    "\n",
    "df_save.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce the DataFrame to a single partition\n",
    "df_save = df_save.coalesce(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_save\n",
    "\n",
    "\n",
    "# sequence lengths for track interactions\n",
    "track_lengths = data.filter(data[\"istrack\"] == 1).groupBy(\"user_id\").count()\n",
    "\n",
    "# sequence lengths for genre interactions\n",
    "genre_lengths = data.filter(data[\"isgenre\"] == 1).groupBy(\"user_id\").count()\n",
    "\n",
    "# sequence lengths for album interactions\n",
    "album_lengths = data.filter(data[\"isalbum\"] == 1).groupBy(\"user_id\").count()\n",
    "\n",
    "# sequence lengths for artist interactions\n",
    "artist_lengths = data.filter(data[\"isartist\"] == 1).repartition(500, \"user_id\").groupBy(\"user_id\").count()\n",
    "\n",
    "\n",
    "def calculate_entropy_udf(track_count, album_count, artist_count, genre_count):\n",
    "    \"\"\"\n",
    "    calculate the entropy of item type distribution.\n",
    "    \"\"\"\n",
    "    counts=np.array([track_count, album_count, artist_count, genre_count])\n",
    "    probs=counts / counts.sum() if counts.sum() > 0 else np.zeros_like(counts)\n",
    "\n",
    "    return float(-np.sum(probs*np.log(probs + 1e-10))) # returns the (Shannon) entropy\n",
    "\n",
    "entropy_udf = F.udf(calculate_entropy_udf, DoubleType())\n",
    "\n",
    "def plot_sequence_distribution_with_entropy(data, album_lengths, track_lengths, genre_lengths, artist_lengths, \n",
    "                                            lower_bound, upper_bound, entropy_threshold):\n",
    "    \"\"\"\n",
    "    Plots the distribution of user ratings sequence lengths filtered by both sequence length and entropy of item\n",
    "    types distribution.\n",
    "    \n",
    "    Also returns the filtered dataset to be saved for modelling.\n",
    "\n",
    "    args:\n",
    "    data (DataFrame): Original dataset.\n",
    "    album_lengths (spark df): spark df containing album counts.\n",
    "    track_lengths (spark df): spark df containing track counts.\n",
    "    genre_lengths (spark df): spark df containing genre counts.\n",
    "    artist_lengths (spark df): spark df containing artist counts.\n",
    "    lower_bound (int): Minimum number of ratings under consideration.\n",
    "    upper_bound (int): Maximum number of ratings under consideration.\n",
    "    entropy_threshold (float): The entropy threshold for filtering.\n",
    "    \n",
    "    Returns:\n",
    "    spark df: Filtered spark df based on user_id where sequence length and entropy match the criteria.\n",
    "    \"\"\"\n",
    "    # rename columns first, before the join\n",
    "    track_lengths=track_lengths.withColumnRenamed(\"count\", \"track_count\")\n",
    "    album_lengths = album_lengths.withColumnRenamed(\"count\", \"album_count\")\n",
    "    artist_lengths = artist_lengths.withColumnRenamed(\"count\", \"artist_count\")\n",
    "    genre_lengths = genre_lengths.withColumnRenamed(\"count\", \"genre_count\")\n",
    "\n",
    "    # merge dfs by userid\n",
    "    merged_df=track_lengths.join(album_lengths, \"user_id\", \"outer\") \\\n",
    "                             .join(artist_lengths, \"user_id\", \"outer\") \\\n",
    "                             .join(genre_lengths, \"user_id\", \"outer\") \\\n",
    "                             .fillna(0)  # NaNs --> 0\n",
    "\n",
    "    # total seq length \n",
    "    merged_df=merged_df.withColumn(\n",
    "        \"count\", F.col(\"track_count\") + F.col(\"album_count\") + F.col(\"artist_count\") + F.col(\"genre_count\")\n",
    "    )\n",
    "\n",
    "    # calculate entropy for item type distribution for @ user\n",
    "    merged_df=merged_df.withColumn( \"entropy\",\n",
    "        entropy_udf(\"track_count\", \"album_count\", \"artist_count\", \"genre_count\")\n",
    "    )\n",
    "\n",
    "    # Apply filters : sequence length and entropy threshold\n",
    "    filtered_df=merged_df.filter((F.col(\"count\") >= lower_bound) & \n",
    "                                   (F.col(\"count\") <= upper_bound) & \n",
    "                                   (F.col(\"entropy\") > entropy_threshold))\n",
    "\n",
    "    df_for_plot=filtered_df.select(\"count\").toPandas()\n",
    "\n",
    "    # Plot the histogram of sequence length distribution filtered data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df_for_plot[\"count\"], bins=50, color='skyblue', alpha=0.7)\n",
    "    plt.xlabel('Number of Ratings per User')\n",
    "    plt.ylabel('Number of Users')\n",
    "    plt.title(f'Distribution of Rating Sequence Lengths (Filtered by Entropy > {entropy_threshold})')\n",
    "    plt.show()\n",
    "\n",
    "    # some helpful results for visualisation\n",
    "    print(f\"Users with sequence lengths between {lower_bound} and {upper_bound} and entropy > {entropy_threshold}: {len(df_for_plot)}\")\n",
    "\n",
    "    # filter original 'data' based on user_id from filtered_df\n",
    "    filtered_user_ids=filtered_df.select(\"user_id\").rdd.flatMap(lambda x: x).collect()  # Collect user_ids to a list\n",
    "    filtered_data=data.filter(F.col(\"user_id\").isin(filtered_user_ids))\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "filtered_data = plot_sequence_distribution_with_entropy(\n",
    "    data, album_lengths, track_lengths, genre_lengths, artist_lengths, \n",
    "    lower_bound=28, upper_bound=170, entropy_threshold=1.3825\n",
    ")\n",
    "\n",
    "filtered_data.show()\n",
    "\n",
    "# Save function to avoid repetition\n",
    "def save_and_confirm(df, path, format='parquet'):\n",
    "    \"\"\"\n",
    "    Save the given DataFrame to the specified path and confirm once saved.\n",
    "    \n",
    "    Parameters:\n",
    "    df (spark df): The spark df to be saved.\n",
    "    path (str): The location to save the DataFrame.\n",
    "    format (str): Format to save the df. Either 'csv' or 'parquet'. Defaults to 'parquet'.\n",
    "    \"\"\"\n",
    "    if format == 'csv': df.write.option(\"header\", \"true\").csv(path)\n",
    "    else: df.write.mode(\"overwrite\").parquet(path)\n",
    "    \n",
    "    print(f\"Data has been saved to {path} in {format} format.\")\n",
    "\n",
    "save_and_confirm(filtered_data, \"../data/development\", format='parquet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henrik/anaconda3/envs/CV-tf/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set the image size and paths\n",
    "imsize = 224\n",
    "image_dir = '../train/'\n",
    "metadata = pd.read_csv('../train.csv')\n",
    "metadata['filename'] = metadata['id'].apply(lambda x: f\"{image_dir}{x}.jpg\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(metadata, test_size=0.2, random_state=42)\n",
    "\n",
    "# Subtract 1 from each label in the 'stable_height' column to make labels zero-indexed\n",
    "train_df['stable_height'] = train_df['stable_height'] - 1\n",
    "val_df['stable_height'] = val_df['stable_height'] - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "random_seed = 25\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "class CustomViTModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CustomViTModel, self).__init__()\n",
    "        # Load the pre-trained Vision Transformer and extract the backbone\n",
    "        self.vit_backbone = ViTForImageClassification.from_pretrained(\n",
    "            \"google/vit-base-patch16-224\",\n",
    "            num_labels=num_labels,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).vit\n",
    "        \n",
    "        # Add custom fully connected layers\n",
    "        self.custom_classifier = nn.Sequential(\n",
    "            nn.Linear(768, 1024),\n",
    "            nn.Linear(1024, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the Vision Transformer backbone\n",
    "        vit_outputs = self.vit_backbone(x)\n",
    "        # Extract the last hidden state (we use the CLS token embedding as the feature)\n",
    "        vit_features = vit_outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        # Forward pass through the custom classifier\n",
    "        output = self.custom_classifier(vit_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, is_train=True):\n",
    "        self.dataframe = dataframe\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['filename']\n",
    "        label = self.dataframe.iloc[idx]['stable_height']\n",
    "        image = Image.open(img_path).convert('RGB')  # Load the image file\n",
    "        \n",
    "        # Resize the image manually\n",
    "        image = image.resize((224, 224))\n",
    "        \n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        image = (image - 127.5) / 127.5  # Normalize the image\n",
    "        \n",
    "        # Convert the numpy array to a PyTorch tensor\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)  # Change HWC to CHW format for PyTorch\n",
    "        \n",
    "        return image, label \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = CustomDataset(train_df, is_train=True)\n",
    "val_dataset = CustomDataset(val_df, is_train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/henrik/anaconda3/envs/CV-tf/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.5569626421978076, Learning Rate: 0.0001\n",
      "Validation Loss: 1.4024630164106686, Accuracy: 41.28%\n",
      "Validation loss decreased (inf --> 1.402463).  Saving model ...\n",
      "Epoch [2/100], Loss: 1.2632704256102443, Learning Rate: 0.0001\n",
      "Validation Loss: 1.229226807753245, Accuracy: 48.50%\n",
      "Validation loss decreased (1.402463 --> 1.229227).  Saving model ...\n",
      "Epoch [3/100], Loss: 0.9331069805969795, Learning Rate: 0.0001\n",
      "Validation Loss: 1.2608084132273991, Accuracy: 49.93%\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch [4/100], Loss: 0.5865986347974589, Learning Rate: 0.0001\n",
      "Validation Loss: 1.5306539634863536, Accuracy: 51.30%\n",
      "EarlyStopping counter: 2 out of 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     64\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 65\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Get current learning rate from optimizer\u001b[39;00m\n\u001b[1;32m     68\u001b[0m current_lr \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pth', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = CustomViTModel(6)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Initialize early stopping and reduce LR on plateau\n",
    "early_stopping = EarlyStopping(patience=7, verbose=True, path='best_model.pth')\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, min_lr=1e-6, verbose=True)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Get current learning rate from optimizer\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}, Learning Rate: {current_lr}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    print(f'Validation Loss: {val_loss}, Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping check\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    # Break if early stopping is triggered\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"../model/visionT.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
